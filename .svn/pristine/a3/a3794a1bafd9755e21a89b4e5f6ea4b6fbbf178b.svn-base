--- linux-2.6.12-rc2-orig/fs/buffer.c	2005-04-04 18:38:24.000000000 +0200
+++ linux-2.6.12-rc2-fkt/fs/buffer.c	2005-05-21 01:11:37.106926520 +0200
@@ -1,3 +1,5 @@
+/*****	09-Aug-99	rdr		add fkt probes *****/
+
 /*
  *  linux/fs/buffer.c
  *
@@ -41,6 +43,9 @@
 #include <linux/bitops.h>
 #include <linux/mpage.h>
 
+/* rdr */
+#include <linux/fkt.h>						       /* rdr */
+
 static int fsync_buffers_list(spinlock_t *lock, struct list_head *list);
 static void invalidate_bh_lrus(void);
 
@@ -142,6 +147,8 @@ void end_buffer_write_sync(struct buffer
 	}
 	unlock_buffer(bh);
 	put_bh(bh);
+	/* st */
+	FKT_PROBE2(FKT_FKT_KEYMASK,FKT_END_IO_SYNC_CODE,atomic_read(&bh->b_count),bh->b_state);
 }
 
 /*
@@ -337,6 +344,9 @@ asmlinkage long sys_fsync(unsigned int f
 	struct address_space *mapping;
 	int ret, err;
 
+	/* rdr */
+	FKT_PROBE1(FKT_FILES_KEYMASK, FKT_FSYNC_ENTRY_CODE, fd);       /* rdr */
+
 	ret = -EBADF;
 	file = fget(fd);
 	if (!file)
@@ -370,6 +380,10 @@ asmlinkage long sys_fsync(unsigned int f
 out_putf:
 	fput(file);
 out:
+
+	/* rdr */
+	FKT_PROBE1(FKT_FILES_KEYMASK, FKT_FSYNC_EXIT_CODE, ret);       /* rdr */
+
 	return ret;
 }
 
@@ -917,6 +931,9 @@ static int fsync_buffers_list(spinlock_t
 	struct list_head tmp;
 	int err = 0, err2;
 
+	/* st */
+	FKT_PROBE0(FKT_FKT_KEYMASK,FKT_FSYNC_BUFFERS_LIST_ENTRY_CODE);
+
 	INIT_LIST_HEAD(&tmp);
 
 	spin_lock(lock);
@@ -956,6 +973,10 @@ static int fsync_buffers_list(spinlock_t
 	
 	spin_unlock(lock);
 	err2 = osync_buffers_list(lock, list);
+
+	/* st */
+	FKT_PROBE1(FKT_FKT_KEYMASK,FKT_FSYNC_BUFFERS_LIST_EXIT_CODE,err?err:err2);
+
 	if (err)
 		return err;
 	else
@@ -2981,6 +3002,68 @@ out:
 }
 EXPORT_SYMBOL(try_to_free_buffers);
 
+/* st: let people determine whether a page's buffers have been written (at least
+ * once) */
+int buffers_dirty(struct page *page) {
+	int dirty = 1;
+	struct buffer_head *tmp, *bh;
+	spinlock_t *bd_lock = &page->mapping->private_lock;
+
+	spin_lock(bd_lock);
+	if (!page_has_buffers(page)) {
+		spin_unlock(bd_lock);
+		return 0;
+	}
+	bh = page_buffers(page);
+	tmp = bh;
+	do {
+		if (test_bit(BH_Dirty,&tmp->b_state) || /* not queued yet */
+			atomic_read(&tmp->b_count) ||
+			test_bit(BH_Lock,&tmp->b_state))
+			goto busy;
+		tmp = tmp->b_this_page;
+	} while (tmp != bh);
+
+	dirty = 0;
+busy:
+	spin_unlock(bd_lock);
+	return dirty;
+}
+EXPORT_SYMBOL(buffers_dirty);
+
+void wait_buffers_clean(struct page *page) {
+	struct buffer_head *tmp, *bh;
+	spinlock_t *bd_lock = &page->mapping->private_lock;
+
+restart:
+	spin_lock(bd_lock);
+	if (!page_has_buffers(page)) {
+		spin_unlock(bd_lock);
+		return;
+	}
+
+	bh = page_buffers(page);
+
+	tmp = bh;
+	do {
+		if (buffer_dirty(tmp)) {
+			spin_unlock(bd_lock);
+			printk(KERN_DEBUG "waiting on dirty buffer %p\n",tmp);
+			wait_on_bit(&bh->b_state, BH_Dirty, sync_buffer, TASK_UNINTERRUPTIBLE);
+			goto restart;
+		}
+		if (atomic_read(&tmp->b_count) ||
+			buffer_locked(tmp)) {
+			spin_unlock(bd_lock);
+			wait_on_buffer(tmp);
+			goto restart;
+		}
+		tmp = tmp->b_this_page;
+	} while (tmp != bh);
+	spin_unlock(bd_lock);
+}
+EXPORT_SYMBOL(wait_buffers_clean);
+
 int block_sync_page(struct page *page)
 {
 	struct address_space *mapping;
